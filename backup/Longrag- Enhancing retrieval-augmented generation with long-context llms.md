# 摘要

传统的RAG检索单位一般很短，以DPR为例，它的检索长度为100个单词，对于整个维基百科语料库来说很像是大海捞针，但是对于生成器来说，它只需要负责根据检索单位生成回答，这样不平衡的设计会导致性能不理想。小单元很容易导致上下文缺失，语义相似度的重复也会在检索阶段插入负样本。但是，生成器的性能并没有得到充分发挥，为了提高平衡程度，作者提出了RAG的新框架： LongRAG， 包含 Long retriever 和 Long reader。 在此之前， 基于维基百科的NQ和HotpotQA平均文档长度在1k token左右，此框架可以处理完整的维基百科数据，并分割为4k 长度的单位，达到了30倍的提升（和100相比）。通过提高单位长度，作者将文档数量从22M降低到600K， 显著减轻retriever压力，并且只检索top 8的结果实现了更好的表现。

# 引言

如果使用长文本单位，那么上下文的信息损失会降低，并且减少了检索次数，同时不需要重排。

# 框架

## Long retriever

对于语料库$\mathcal{C}$ ，long RAG和普通的做法并无区别， 即将文档库$\mathcal{C}$ 和查询$q$ 结合， 通过函数$\mathcal{F}$ 得到检索的文档 $\mathcal{C}_\mathcal{F}$ ， 区别之处在于$\mathcal{C}_\mathcal{F}$的长度，一般RAG的长度在几百个token，而longRAG通常超过4k tokens。 对于检索函数$\mathcal{F}$ ，由下面三部分构成：

1. 制定检索单元：
    

在语料库上应用一个函数做预处理，这里我的感觉类似于chunking， 传统RAG的size在几百token左右，longRAG的size可以是整篇文章，或者是多篇文章。如果原文档长度超过4k，则全部保留，如果少于4k，则将相关文章拼在一起。这么做有两个优势： 首先是generator输入数量少了，其次是上下文信息保留完整。

2\. 相似度搜索

不同之处在于，选了两种Encoder，因为query和文档的长度量级不同。

$$sim(q, g) = E_Q(q)^T E_C(g)$$

作者采用多种切分策略，将g 切分为 512 / 4k / 保留全部token的方法，选择其中相似度最大的。

3\. 聚合

## Long reader

用Gemini-1.5 pro / GPT 4o

我们对短上下文和长上下文采用不同的方法。对于通常包含少于 1K 个标记的短上下文，我们指示阅读器直接从从语料库中检索到的提供上下文中生成答案。对于通常长于 4K 个标记的长上下文，我们根据经验发现，使用与短上下文类似的提示（模型直接从长上下文中提取最终答案）通常会导致性能下降。相反，最有效的方法是利用 LLM 作为聊天模型。最初，它会输出一个长答案，通常包含几个单词到几个句子。随后，我们提示它通过进一步从长答案中提取简短答案来生成简短答案。

# 实验

## 数据集

## 实验数据

在NQ数据集的测试，注意到Corpus Size 如果使用聚合方法，会压缩到600K，召回的文档包含答案的概率会更高

接入下游大模型之后的性能